{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import dict_to_spark_row\n",
    "import numpy as np\n",
    "from petastorm.unischema import Unischema, UnischemaField\n",
    "from pyspark.sql.types import IntegerType,StringType,DoubleType\n",
    "from petastorm.codecs import ScalarCodec\n",
    "\n",
    "import pyarrow.dataset as ds\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: <pyspark.sql.session.SparkSession object at 0x7f739f5aba30>\n",
      "parquet data type: [('c_custkey', 'bigint'), ('c_name', 'string'), ('c_address', 'string'), ('c_nationkey', 'bigint'), ('c_phone', 'string'), ('c_acctbal', 'double'), ('c_mktsegment', 'string'), ('c_comment', 'string')]\n",
      "parquet schema: StructType([StructField('c_custkey', LongType(), True), StructField('c_name', StringType(), True), StructField('c_address', StringType(), True), StructField('c_nationkey', LongType(), True), StructField('c_phone', StringType(), True), StructField('c_acctbal', DoubleType(), True), StructField('c_mktsegment', StringType(), True), StructField('c_comment', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"ParquetToDataset\").getOrCreate()\n",
    "print(f\"spark: {spark}\")\n",
    "parquet_data = spark.read.parquet(\"/mnt/cephfs/tpch_sf2_parquet/customer\")\n",
    "print(f\"parquet data type: {parquet_data.dtypes}\")\n",
    "schema = parquet_data.schema\n",
    "print(f\"parquet schema: {parquet_data.schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=ParquetToDataset>\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc1: <SparkContext master=local[*] appName=ParquetToDataset>\n"
     ]
    }
   ],
   "source": [
    "spark1 = SparkSession.builder.config('spark.driver.memory', '2g').master('local[2]').getOrCreate()\n",
    "sc1 = spark.sparkContext\n",
    "print(f\"sc1: {sc1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_generator(x):\n",
    "    \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\n",
    "    return {'id': x,\n",
    "            'image1': np.random.randint(0, 255, dtype=np.uint8, size=(128, 256, 3)),\n",
    "            'array_4d': np.random.randint(0, 255, dtype=np.uint8, size=(4, 128, 30, 3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_generator(x):\n",
    "    \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\n",
    "    return {'id': x,\n",
    "            'image1': np.random.randint(0, 255, dtype=np.uint8, size=(128, 256, 3)),\n",
    "            'array_4d': np.random.randint(0, 255, dtype=np.uint8, size=(4, 128, 30, 3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomerSchema = Unischema('CustomerSchema', [\n",
    "    UnischemaField('c_custkey', np.int32, (), ScalarCodec(IntegerType()), False),\n",
    "    UnischemaField('c_name', np.str_, (), ScalarCodec(StringType()), False),\n",
    "    UnischemaField('c_address', np.str_, (), ScalarCodec(StringType()), False),\n",
    "    UnischemaField('c_nationkey', np.int32, (), ScalarCodec(IntegerType()), False),\n",
    "    UnischemaField('c_phone', np.str_, (), ScalarCodec(StringType()), False),\n",
    "    UnischemaField('c_acctbal', np.float64, (), ScalarCodec(DoubleType()), False),\n",
    "    UnischemaField('c_mktsegment', np.str_, (), ScalarCodec(StringType()), False),\n",
    "    UnischemaField('c_comment', np.str_, (), ScalarCodec(StringType()), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yue21/petastorm/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem = pyarrow.localfs\n",
      "/home/yue21/petastorm/petastorm/fs_utils.py:89: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem_factory = lambda: pyarrow.localfs\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Passed non-file path: /mnt/cephfs/test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mparquet(\u001b[39m'\u001b[39m\u001b[39m/mnt/cephfs/tpch_sf2_parquet/customer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m materialize_dataset(spark, output_url, CustomerSchema):\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[39m# rows_rdd = sc.parallelize(range(rows_count))\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m     \u001b[39m# spark.createDataFrame(rows_rdd, schema.as_spark_schema()) \\\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     df\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mmode(\u001b[39m'\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mparquet(\u001b[39m'\u001b[39m\u001b[39mfile:///home/yue21/test\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:120\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    121\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/petastorm/petastorm/etl/dataset_metadata.py:107\u001b[0m, in \u001b[0;36mmaterialize_dataset\u001b[0;34m(spark, dataset_url, schema, row_group_size_mb, use_summary_metadata, filesystem_factory)\u001b[0m\n\u001b[1;32m    104\u001b[0m     dataset_path \u001b[39m=\u001b[39m get_dataset_path(urlparse(dataset_url))\n\u001b[1;32m    105\u001b[0m filesystem \u001b[39m=\u001b[39m filesystem_factory()\n\u001b[0;32m--> 107\u001b[0m dataset \u001b[39m=\u001b[39m pq\u001b[39m.\u001b[39;49mParquetDataset(\n\u001b[1;32m    108\u001b[0m     dataset_path,\n\u001b[1;32m    109\u001b[0m     filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m    110\u001b[0m     validate_schema\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    112\u001b[0m _generate_unischema_metadata(dataset, schema)\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_summary_metadata:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/parquet.py:1332\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, read_dictionary, memory_map, buffer_size, partitioning, use_legacy_dataset, pre_buffer, coerce_int96_timestamp_unit)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mmemory_map \u001b[39m=\u001b[39m memory_map\n\u001b[1;32m   1327\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mbuffer_size \u001b[39m=\u001b[39m buffer_size\n\u001b[1;32m   1329\u001b[0m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pieces,\n\u001b[1;32m   1330\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partitions,\n\u001b[1;32m   1331\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommon_metadata_path,\n\u001b[0;32m-> 1332\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata_path) \u001b[39m=\u001b[39m _make_manifest(\n\u001b[1;32m   1333\u001b[0m      path_or_paths, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fs, metadata_nthreads\u001b[39m=\u001b[39;49mmetadata_nthreads,\n\u001b[1;32m   1334\u001b[0m      open_file_func\u001b[39m=\u001b[39;49mpartial(_open_dataset_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_metadata)\n\u001b[1;32m   1335\u001b[0m )\n\u001b[1;32m   1337\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommon_metadata_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommon_metadata_path) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/parquet.py:1579\u001b[0m, in \u001b[0;36m_make_manifest\u001b[0;34m(path_or_paths, fs, pathsep, metadata_nthreads, open_file_func)\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m path_or_paths:\n\u001b[1;32m   1578\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fs\u001b[39m.\u001b[39misfile(path):\n\u001b[0;32m-> 1579\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mPassed non-file path: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m   1580\u001b[0m                       \u001b[39m.\u001b[39mformat(path))\n\u001b[1;32m   1581\u001b[0m     piece \u001b[39m=\u001b[39m ParquetDatasetPiece\u001b[39m.\u001b[39m_create(\n\u001b[1;32m   1582\u001b[0m         path, open_file_func\u001b[39m=\u001b[39mopen_file_func)\n\u001b[1;32m   1583\u001b[0m     pieces\u001b[39m.\u001b[39mappend(piece)\n",
      "\u001b[0;31mOSError\u001b[0m: Passed non-file path: /mnt/cephfs/test"
     ]
    }
   ],
   "source": [
    "# Wrap dataset materialization portion. Will take care of setting up spark environment variables as\n",
    "# well as save petastorm specific metadata\n",
    "rows_count = 10\n",
    "output_url = 'file:///mnt/cephfs/test'\n",
    "rowgroup_size_mb = 256\n",
    "\n",
    "# Read the Parquet file\n",
    "df = spark.read.parquet('/mnt/cephfs/tpch_sf2_parquet/customer')\n",
    "with materialize_dataset(spark, output_url, CustomerSchema):\n",
    "\n",
    "    # rows_rdd = sc.parallelize(range(rows_count))\\\n",
    "    #     .map(row_generator)\\\n",
    "    #     .map(lambda x: dict_to_spark_row(schema, x))\n",
    "\n",
    "    # spark.createDataFrame(rows_rdd, schema.as_spark_schema()) \\\n",
    "    df.write.mode('overwrite').parquet('file:///home/yue21/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    # dataset_path = str(sys.argv[1])\n",
    "    dataset_path = \"file:///home/yue21/test\"\n",
    "    # query_no = int(sys.argv[2])\n",
    "    query_no = 22\n",
    "    # format = str(sys.argv[3])\n",
    "    format = ds.SkyhookFileFormat(\"parquet\", \"/etc/ceph/ceph.conf\")\n",
    "\n",
    "    data = list()\n",
    "    lineitem = ds.dataset((dataset_path), format=format)\n",
    "    # supplier = ds.dataset(os.path.join(dataset_path, \"supplier\"), format=format)\n",
    "    # customer = ds.dataset(os.path.join(dataset_path, \"customer\"), format=format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
